We organize our results into two subsections, with each subsection focusing on results from a phase of experiments.
\vspace{-2mm}
\subsection{Phase-I Results Using Cloverleaf3D Simulation Code}
\label{sec:experiment1}
We report the results of \textbf{Phase-I} experiments that include a weak scaling study, \textbf{C-I}, and \textbf{C-II}.
\vspace{-2mm}
\subsubsection{Weak Scaling Study} 
We considered 17 configurations of the Cloverleaf3D simulation in our weak scaling \textbf{Phase-I} study. 
%
Our experiment configurations span $81^{3}$ cells across 2 MPI ranks on 1 CN to $812^{3}$ cells across 2048 MPI ranks on 512 CNs, with each MPI rank operating on an approximately $64^{3}$ grid.
%
In each case, one GPU was assigned to every MPI rank, and we varied the number of ranks per CN.
%
With respect to the number of MPI ranks on each CN, we ran 1 test using 2 ranks, 10 tests using 4 ranks, and 6 tests using 6 ranks.
%
For each run, we terminated the simulation after 500 cycles.
%
Given the variation in grid size, each test reached a different stage of the simulation.
%
As the simulation grid size increased, the step size used by the simulation decreased.
%
With respect to parameterization of the Lagrangian flow map computation, we used a storage interval of 25 cycles and a data reduction of 1:8 ($\approx32,768$ particles seeded per MPI rank) for every test.

\textbf{Execution Time.} Figure~\ref{fig:total_time} compares the average total time required per step by Lagrangian$_{Dist}$ and Lagrangian$_{Local}$.
%
As the scale of the simulation increased, the cost of communication dominated the execution time for Lagrangian$_{Dist}$.  
%
In comparison, Lagrangian$_{Local}$ scaled better because each MPI rank operated independently.
%
For the range of configurations considered, Lagrangian$_{Local}$ demonstrated up to 4x speed-up over Lagrangian$_{Dist}$.
%
Further, we expect this speed-up would increase even more as the scale of the simulation increases.
%
The cost of particle advection (for both techniques) increased as the scale of the simulation increased.
%
However, this increase in particle advection cost was attributed to the difference in each test domain and the RK4 kernel that performed velocity field interpolation.
%
Further, use of a faster particle advection kernel would result in greater speed-ups for Lagrangian$_{Local}$.
\input{weakscaling.tex}
\input{gpu_nodes.tex}


The ``sawtooth'' nature of the line curves in Figure~\ref{fig:total_time} is the result of a series of test configurations alternating between 4 and 6 ranks per CN.
%
Varying the number of ranks (each using 1 GPU) on each CN impacted both particle advection and communication costs.
%
Figure~\ref{fig:gpu_nodes} isolates these costs to show the weak scaling trends.
%
From Figure~\ref{fig:advection}, particle advection performed better with 4 GPUs per CN versus 6.
%
Use of shared memory by multiple GPUs on a single CN and saturation of the NVLink by the VTK-m particle advection kernel caused this effect.
%
In contrast, the MPI communication cost showed a reduction when using 6 ranks versus 4 per CN as seen in Figure~\ref{fig:communication}. 
%
On-node MPI communication optimizations contributed to better performance when grouping a larger number of MPI ranks on each CN. 
%
However, the communication cost of inter-node remained high in comparison to intra-node.
%
Configurations using 4 ranks per CN showed particularly poor scalability as the number of CNs increased.
%

\textbf{Reconstruction Accuracy.}
We calculated the reconstruction accuracy for 8 of the Lagrangian$_{Local}$ weak scaling \textbf{Phase-I} configurations, which we labeled T1 through T8~(see Figure~\ref{fig:clover_plot}). 
%
Configurations terminated 2\%-5\% of particles and consequently stored 95\%-98\% of all initially seeded trajectories. 
%
We measured the accuracy of reconstructing the discarded trajectories only and present the results using violin plots in Figure~\ref{fig:clover_plot}.
%
As the simulation grid resolution increased, the grid cell size and particle advection step size decreased, while the total number of particles sampling the domain increased. 
%
Over 75\% of discarded trajectories have reconstruction error under 25\% of a GCS from the ground truth.
%
Further, this trend was observed across all the 8 test configurations we reconstructed. 

%
Overall, using Lagrangian$_{Local}$ with a storage interval of 25 cycles and 1:8 data reduction factor, the complete Lagrangian flow map was reconstructed accurately (under 100\% of GCS) while providing speed-ups of 2x-4x for the Cloverleaf3D data set.
\input{clover_plot.tex}

\vspace{-2mm}
\subsubsection{C-I Results} 
%
\input{strongscaling.tex}
%
Multiphysics HPC simulations typically have millions of grid points per rank and increase grid resolution as the number of ranks increases. 
%
To identify limitations, we evaluated Lagrangian$_{Local}$ in situations where this was not the case, i.e., when the number of processes operating over a fixed grid size increased.
%

Since the Lagrangian$_{Local}$ strategy was to discard trajectories exiting the rank-specific domain, this approach was susceptible to low-resolution data blocks (i.e, sampling would use a small number of particles per rank) and longer storage intervals (i.e., integration times).
%
This experiment measured the error of $1,000$ new pathlines generated for 800 cycles compared to the ground truth. 
%
Further, three parameter options were considered for domain decomposition, storage interval, and data reduction factor. 
%
Figure~\ref{fig:strongscaling} shows the pathline error when interpolating the flow maps generated by Lagrangian$_{Dist}$ and Lagrangian$_{Local}$. 
%
The accuracy of Lagrangian$_{Local}$ remained close to Lagrangian$_{Dist}$ until the domain decomposition was at its highest~($64^{3}$ grid decomposed across 32 ranks).
%
%
Although the overall accuracy of the interpolated pathlines was high (within a single GCS on average for all tests), both techniques lost some accuracy as the storage interval and data reduction factor increased, with Lagrangian$_{Local}$ performing worse under greater domain decomposition.
%
These results highlight the limitations of the Lagrangian$_{Local}$ technique as is.
%

\vspace{-2mm}
\subsubsection{C-II Results} 
Here, we compared Lagrangian$_{Local}$ to an Eulerian representation with temporal subsampling.
%
Table~\ref{clover_eul_table} shows the results of \textbf{C-II}.
%
We considered three storage intervals: 20, 40, and 60.
%
We used 96 MPI ranks distributed across 16 CNs and a grid size of $586^3$.
%
Lagrangian$_{Local}$ used a data reduction of 1:8, whereas for the Eulerian technique we stored the full spatial resolution.
%
To compare accuracy, we reconstructed 100,000 randomly seeded pathlines for 600 cycles.
%
%
Overall, Lagrangian$_{Local}$ was increasingly accurate (6x to 11x) compared to the Eulerian approach as the interval size increased, but required less data storage.
%
These results aligned with findings in prior works~\cite{agranovsky2014improved, sane2018revisiting} that compared the use of Lagrangian representations to the traditional approach under sparse temporal settings.
\input{eulerian_table.tex}

\vspace{-2mm}
\subsection{Phase-II Results Using ABC, Nyx, Jet Data Sets}
This section presents results for three time-varying data sets.
%
We considered a fixed amount of compute resources and varied configuration parameters that impact both execution time and reconstruction accuracy.
%
Figure~\ref{fig:dataset_timings} contains the computation costs for all \textbf{Phase-II} experiments.
%
{Additionally, we derived, visualized, and compared the finite-time Lyapunov exponent (FTLE) scalar field using select Lagrangian$_{Local}$ flow maps for each of these data sets to the ground truth FTLE field.}
\input{dataset_timings.tex}
\label{sec:experiment2}
\vspace{-2mm}
\subsubsection{ABC Flow}
\label{sec:abc}

Table~\ref{abc_tab} lists all the ABC \textbf{Phase-II} test configurations.
%
\input{abc_table.tex}
\input{abc_plot.tex}

\textbf{Execution Time.} Using 64 MPI ranks across 16 CNs, each with 1 GPU, Lagrangian$_{Local}$ required up to 2.8x less execution time as Lagrangian$_{Dist}$.
%
Figure~\ref{fig:dataset_timings} shows particle advection and communication costs are directly proportional to the particle count.
%
Further, particle advection for up to 2M particles~(32k particles per GPU) was performed in under 0.004 seconds per step for all data sets.
%
\input{abc_maps.tex}

\textbf{Reconstruction Accuracy.} Figure~\ref{fig:abc_plot}'s violin plots indicate the ABC data set showed shifts in reconstruction error distribution when varying the storage intervals and sampling resolution. 
%
The error range for all configurations, however, was similar.
%
The discarded trajectories from the ABC data set Lagrangian flow map were reconstructed accurately, with trajectories interpolated to the same grid cell as the ground truth.
%
Specifically, for all 9 tests, the mean reconstruction error was between 2.5\% and 5\% of GCS from the ground truth.
%
{Figure~\ref{abc_ftle} visualizes the FTLE field derived from the reduced Lagrangian$_{Local}$ test T6. 
%
Although reconstruction near extrema is similar to the ground truth, the FTLE is overestimated in other regions and artifacts are visible along node boundaries.}
\input{abc_figure.tex}

\textbf{Impact of Varying Storage Interval.} Figure~\ref{fig:abc_map} shows heatmaps of the reconstruction error across all intervals for tests T4 and T6.
%
For a straightforward comparison, the figures show the absolute number of particles reconstructed for each storage interval.
%
Comparing these heatmaps, the effect of increasing the storage interval was that a larger number of particles were terminated in a single interval, and thus reconstruction error increased. 
%

\vspace{-2mm}
\subsubsection{Nyx Cosmology}
\label{sec:nyx}
Table~\ref{nyx_tab} lists all the Nyx \textbf{Phase-II} test configurations. 
%
\input{nyx_table.tex}

\textbf{Execution Time.} Across our 8 tests using the Nyx data set, Lagrangian$_{Local}$ computed a flow map up to 5.2x faster than the corresponding Lagrangian$_{Dist}$ flow map.
%
Additionally, Figure~\ref{fig:dataset_timings} shows the standard deviation was greater for the Lagrangian$_{Dist}$ tests and was caused by the larger number of particles exchanges between ranks for this data set.
%
\input{nyx_plot.tex}
%\input{nyx_map.tex}
\input{nyx_figure.tex}

\textbf{Reconstruction Accuracy.} As a consequence of a large number of particles being discarded by Lagrangian$_{Local}$, the accuracy of reconstruction was impacted for tests with longer storage intervals.
%
7 of 8 tests showed up to the third quartile reconstructing under 100\% of a GCS. 
%
Comparing tests T3 and T4 (1:1 data reduction factor) to tests T7 and T8 (1:8 data reduction factor), although T3 and T4 discarded a greater percentage of samples, they remained more accurate due to the absolute number of particles used to sample the domain.
%
For shorter storage interval lengths (T1, T2, T5, T6), the reconstruction quality was high for both data reduction factors.
%
{Figure~\ref{nyx_ftle} compares the FTLE field derived by interpolating a single longer interval~(T3) to interpolating multiple intervals~(T1) of Lagrangian$_{Local}$ flow maps.
%
Interpolating multiple intervals leads to error propagation.
%
The consequence of the error propagation was the noise introduced in the FTLE derived using T1 flow maps.
%
In contrast, interpolating the single longer interval of T3 flow maps generated an FTLE field closer to the ground truth.
%
}
%



\vspace{-2mm}
\subsubsection{Jet Flow}
\label{sec:jet}
Table~\ref{jet_tab} lists all the Jet \textbf{Phase-II} test configurations.
%
\input{jet_table.tex}

\textbf{Execution Time.} For the Jet data set, Lagrangian$_{Local}$ computed a flow map up to 3.9x faster than the corresponding Lagrangian$_{Dist}$ flow map.
%
For this data set, we considered shorter storage intervals, and thus a smaller percentage of particles required particle exchange to continue trajectory integration.
%
However, Figure~\ref{fig:dataset_timings} shows the variability in the cost of communication as it was susceptible to network usage and bandwidth contention.
%
A single outlier Lagrangian$_{Local}$ test showed a higher computation cost. 

\input{jet_plot.tex}
\input{jet_map.tex}
\textbf{Reconstruction Accuracy.} The Jet data set presented an adversarial case for our proposed optimization of computing local flow maps.
%
The data set contained regions with high velocity magnitude.
%
Across the range of configurations in Figure~\ref{fig:jet_plot}, both the storage interval and the data reduction factor impacted the reconstruction accuracy.
%
6 of 9 tests had a mean reconstruction error under 100\% of GCS. 
%
Further investigation into the distribution of T2 revealed that the longer storage interval and 1:1 sampling resulted in several particles terminating in easy-to-reconstruct areas of the domain at the start of the simulation (most other configurations showed particle termination only after cycle 30).
%
For tests using a storage interval of 10~(T2, T4, T6, T8), the reconstruction accuracy is reduced.
%


\textbf{Impact of Varying Data Reduction Factor.} Figure~\ref{fig:jet_map} compares the distribution of reconstruction error as the data reduction factor increases. 
%
Each cell in the heatmap shows a percentage of particles to enable comparison between tests.
%
For test T1 using a 1:1 sampling, reconstruction accuracy was high and a larger percentage of particles were reconstructed with error under 100\% of GCS.
%
As the number of particles used to sample the domain reduced, a higher percentage of particles were reconstructed multiple cells away from the ground truth. 
%
In Figure~\ref{fig:jet_7}, test T7 used a 1:64 data reduction factor and there are more red/orange cells with higher error during the later stages of the simulation. 
%
{Figure~\ref{jet_ftle} visualizes an FTLE field derived using the T1~(1:1) and T3~(1:8) Lagrangian$_{Local}$ flow maps.
%
Although for both tests the overall FTLE ridge structure can be visualized, for T3 using fewer particles, the loss of accuracy due to discarded trajectories is more evident. 
%
}
