We organize our results into two subsections, with each subsection focusing on results from a phase of experiments.
%\vspace{-2mm}
\subsection{Phase-I Results Using Cloverleaf3D Simulation Code}
\label{sec:experiment1}
We report the results of \textbf{Phase-I} experiments that include a weak scaling study, \textbf{C-I}, and \textbf{C-II}.
%\vspace{-2mm}
\subsubsection{Weak Scaling Study} 
We considered 17 configurations of the Cloverleaf3D simulation in our weak scaling \textbf{Phase-I} study. 
%
Our experiment configurations span $81^{3}$ cells across 2 MPI ranks on 1 CN to $812^{3}$ cells across 2048 MPI ranks on 512 CNs, with each MPI rank operating on an approximately $64^{3}$ grid.
%
In each case, one GPU was assigned to every MPI rank, and we varied the number of ranks per CN.
%
With respect to the number of MPI ranks on each CN, we ran 1 test using 2 ranks, 10 tests using 4 ranks, and 6 tests using 6 ranks.
%
For each run, we terminated the simulation after 500 cycles.
%
Given the variation in grid size, each test reached a different stage of the simulation.
%
As the simulation grid size increased, the step size used by the simulation decreased.
%
%We believe this was an acceptable assumption for most simulation codes.
%
With respect to parameterization of the Lagrangian flow map computation, we used a storage interval of 25 cycles and a data reduction of 1:8 ($\approx32,768$ particles seeded per MPI rank) for every test.

\textbf{Execution Time.} Figure~\ref{fig:total_time} compares the average total time required per step by Lagrangian$_{Dist}$ and Lagrangian$_{Local}$.
%
As the scale of the simulation increased, the cost of communication dominated the execution time for Lagrangian$_{Dist}$.  
%
In comparison, Lagrangian$_{Local}$ scaled better because each MPI rank operated independently.
%
For the range of configurations considered, Lagrangian$_{Local}$ demonstrated up to 4x speed-up over Lagrangian$_{Dist}$.
%
Further, we expect this speed-up would increase even more as the scale of the simulation increases.
%
The cost of particle advection (for both techniques) increased as the scale of the simulation increased.
%
However, this increase in particle advection cost was attributed to the difference in each test domain and the RK4 kernel that performed velocity field interpolation.
%
%We believe the performance of the kernel considering the parameters of cell size, step size, and vector field is worth future investigation.
%
Further, use of a faster particle advection kernel would result in greater speed-ups for Lagrangian$_{Local}$.
\input{weakscaling.tex}
\input{gpu_nodes.tex}

%\textbf{Varying GPUs/Ranks per Compute Node.}
%Since each CN on Summit has 6 GPUs, a simulation can be configured in several ways.
%
%In our tests, each rank has a dedicated GPU and operates on approximately the same number of particles.
%
The ``sawtooth'' nature of the line curves in Figure~\ref{fig:total_time} is the result of a series of test configurations alternating between 4 and 6 ranks per CN.
%
Varying the number of ranks (each using 1 GPU) on each CN impacted both particle advection and communication costs.
%
%Using data points from our weak scaling \textbf{Phase-I} study, we analyze these effects.
%
%
%Figure~\ref{fig:advection} isolates the costs of particle advection per step of Lagrangian$_{Local}$.
%
Particle advection performed better with 4 GPUs per CN versus 6.
%
Use of shared memory by multiple GPUs on a single CN and saturation of the NVLink by the VTK-m particle advection kernel caused this effect.
%
%
%Figure~\ref{fig:communication} shows the difference in communication costs per step for Lagrangian$_{Dist}$.
%
In contrast, the MPI communication cost showed a reduction when using 6 ranks versus 4 per CN. 
%
%The data points using 4 ranks per CN show poor scalability as the number of CNs continues to increase.
%
On-node MPI communication optimizations contributed to better performance when grouping a larger number of MPI ranks on each CN. 
%
However, the communication cost of inter-node remained high in comparison to intra-node.
%
Configurations using 4 ranks per CN showed particularly poor scalability as the number of CNs increased.
%
%Although these results inform computational performance, the specific configuration is determined by the simulation.
%
These weak scaling trends can be observed by isolating the individual costs. 
%
Figure~\ref{fig:advection} shows particle advection costs per step for Lagrangian$_{Local}$, and Figure~\ref{fig:communication} shows communication costs per step for Lagrangian$_{Dist}$. 

\textbf{Reconstruction Accuracy.}
We calculated the reconstruction accuracy for 8 of the Lagrangian$_{Local}$ weak scaling \textbf{Phase-I} configurations, which we labeled T1 through T8~(see Figure~\ref{fig:clover_plot}). 
%
Configurations terminated 2\%-5\% of particles and consequently stored 95\%-98\% of all initially seeded trajectories. 
%
We measured the accuracy of reconstructing the discarded trajectories only and present the results using violin plots in Figure~\ref{fig:clover_plot}.
%
As the simulation grid resolution increased, the grid cell size and particle advection step size decreased, while the total number of particles sampling the domain increased. 
%
Over 75\% of discarded trajectories have reconstruction error under 25\% of a GCS from the ground truth.
%
Further, this trend was observed across all the 8 test configurations we reconstructed. 

Reconstruction accuracy, however, was not constant across all storage intervals and demonstrates temporal patterns.
%
%
Figure~\ref{fig:clover_map} uses a heatmap for reconstruction accuracy across every interval of a Cloverleaf3D test. 
%
Measuring reconstruction accuracy for individual intervals could provide insight into the suitability to compute local Lagrangian flow maps versus incur communication costs to retain all particles trajectories.
%
%This concept can extend to consider spatial patterns as well. 
%
\input{clover_plot.tex}
\input{clover_map.tex}
%
Overall, using Lagrangian$_{Local}$ with a storage interval of 25 cycles and 1:8 data reduction factor, the complete Lagrangian flow map was reconstructed accurately (under 100\% of GCS) while providing speed-ups of 2x-4x for the Cloverleaf3D data set.

%\vspace{-2mm}
\subsubsection{C-I Results} 
%
\input{strongscaling.tex}
%
Multiphysics HPC simulations typically have millions of grid points per rank and increase grid resolution as the number of ranks increases. 
%
To identify limitations, we evaluated Lagrangian$_{Local}$ in situations where this was not the case, i.e., when the number of processes operating over a fixed grid size increased.
%
%Although multiphysics HPC simulations typically have hundreds of thousands of grid points per data block, we evaluate Lagrangian$_{Local}$ in situations where this is not the case and the number of processes operating over a fixed grid increases .
%

Since the Lagrangian$_{Local}$ strategy was to discard trajectories exiting the rank-specific domain, this approach was susceptible to low-resolution data blocks (i.e, sampling would use a small number of particles per rank) and longer storage intervals (i.e., integration times).
%
This experiment measured the error of $1,000$ new pathlines generated for 800 cycles compared to the ground truth. 
%
Further, three parameter options were considered for domain decomposition, storage interval, and data reduction factor. 
%
Figure~\ref{fig:strongscaling} shows the pathline error when interpolating the flow maps generated by Lagrangian$_{Dist}$ and Lagrangian$_{Local}$. 
%
The accuracy of Lagrangian$_{Local}$ remained close to Lagrangian$_{Dist}$ until the domain decomposition was at its highest~($64^{3}$ grid decomposed across 32 ranks).
%
%
Although the overall accuracy of the interpolated pathlines was high (within a single GCS on average for all tests), both techniques lost some accuracy as the storage interval and data reduction factor increased, with Lagrangian$_{Local}$ performing worse under greater domain decomposition.
%
These results highlight the limitations of the Lagrangian$_{Local}$ technique as is.
%
%That said, we discuss approaches that could mitigate these limitations in Section~\ref{sec:discussion}.

%\vspace{-2mm}
\input{eulerian_table.tex}
\subsubsection{C-II Results} 
Here, we compared Lagrangian$_{Local}$ to an Eulerian representation with temporal subsampling.
%
Table~\ref{clover_eul_table} shows the results of \textbf{C-II}.
%
We considered three storage intervals: 20, 40, and 60.
%
We used 96 MPI ranks distributed across 16 CNs and a grid size of $586^3$.
%
Lagrangian$_{Local}$ used a data reduction of 1:8, whereas for the Eulerian technique we stored the full spatial resolution.
%
To compare accuracy, we reconstructed 100,000 randomly seeded pathlines for 600 cycles.
%
%
Overall, Lagrangian$_{Local}$ was increasingly accurate (6x to 11x) compared to the Eulerian approach as the interval size increased, but required less data storage.
%
These results aligned with findings in prior works~\cite{agranovsky2014improved, sane2018revisiting} that compared the use of Lagrangian representations to the traditional approach under sparse temporal settings.

%\vspace{-2mm}
\subsection{Phase-II Results Using ABC, Nyx, Jet Data Sets}
This section presents results for three time-varying data sets.
%
We considered a fixed amount of compute resources and varied configuration parameters that impact both execution time and reconstruction accuracy.
%
Figure~\ref{fig:dataset_timings} contains the computation costs for all \textbf{Phase-II} experiments.
\input{dataset_timings.tex}
\label{sec:experiment2}
%\vspace{-2mm}
\subsubsection{ABC Flow}
\label{sec:abc}

Table~\ref{abc_tab} lists all the ABC \textbf{Phase-II} test configurations.
% and Figure~\ref{fig:abc_plot} shows the distribution of trajectory reconstruction error for each test.
%
\input{abc_table.tex}
\input{abc_plot.tex}

\textbf{Execution Time.} Using 64 MPI ranks across 16 CNs, each with 1 GPU, Lagrangian$_{Local}$ required up to 2.8x less execution time as Lagrangian$_{Dist}$.
%
Figure~\ref{fig:dataset_timings} shows particle advection and communication costs are directly proportional to the particle count.
%
Further, particle advection for up to 2M particles~(32k particles per GPU) was performed in under 0.004 seconds per step for all data sets.
%
\input{abc_maps.tex}

\textbf{Reconstruction Accuracy.} Figure~\ref{fig:abc_plot}'s violin plots indicate the ABC data set showed shifts in reconstruction error distribution when varying the storage intervals and sampling resolution. 
%
Further, the error range for all configurations was similar.
%
Overall, the discarded trajectories from the ABC data set Lagrangian flow map were reconstructed accurately, with trajectories interpolated to the same grid cell as the ground truth.
%
Specifically, for all 9 tests, the mean reconstruction error was between 2.5\% and 5\% of GCS from the ground truth.
%

\textbf{Impact of Varying Storage Interval.} Figure~\ref{fig:abc_map} shows heatmaps of the reconstruction error across all intervals for tests T4 and T6.
%
The figure shows the absolute number of particles reconstructed for each storage interval for a more straightforward comparison between heatmaps.
%
Comparing these heatmaps, the effect of increasing the storage interval was that a larger number of particles were terminated in a single interval, and thus reconstruction error increased. 
%
%Further, the reconstruction error distribution matched the corresponding violin plots in Figure~\ref{fig:abc_plot}. 
%
Figure~\ref{abc_ftle} visualizes an FTLE field derived using the T6 Lagrangian$_{Local}$ flow map and annotates a discontinuity in the FTLE field caused by discarded particles.
%Even for longer storage intervals, the error remained small, and the majority discarded trajectories were reconstructed with high accuracy for the ABC data set.

%\vspace{-2mm}
\subsubsection{Nyx Cosmology}
\label{sec:nyx}
Table~\ref{nyx_tab} lists all the Nyx \textbf{Phase-II} test configurations. 
%, Figure~\ref{fig:dataset_timings} contains computation costs, and Figure~\ref{fig:nyx_plot} shows the distribution of trajectory reconstruction error for each test.
%
\input{nyx_table.tex}

\textbf{Execution Time.} Across our 8 tests using the Nyx data set, Lagrangian$_{Local}$ computed a flow map up to 5.2x faster than the corresponding Lagrangian$_{Dist}$ flow map.
%
Additionally, Figure~\ref{fig:dataset_timings} shows the standard deviation was greater for the Lagrangian$_{Dist}$ tests and was caused by the larger number of particles exchanges between ranks for this data set.
%

\textbf{Reconstruction Accuracy.} As a consequence of a large number of particles being discarded by Lagrangian$_{Local}$, the accuracy of reconstruction was impacted for tests with longer storage intervals.
%
7 of 8 tests showed up to the third quartile reconstructing under 100\% of a GCS. 
%
Comparing tests T3 and T4 (1:1 data reduction factor) to tests T7 and T8 (1:8 data reduction factor), although T3 and T4 discarded a greater percentage of samples, they remained more accurate due to the absolute number of particles used to sample the domain.
%
For shorter storage interval lengths (T1, T2, T5, T6), the reconstruction quality was high for both data reduction factors.
%
Figure~\ref{nyx_ftle} shows an FTLE field derived using the T1 Lagrangian$_{Local}$ flow map. 
%
Lastly, Figure~\ref{fig:nyx_map} shows the distribution of error and temporal pattern of communication during the simulation. 
% of a Nyx configuration.

\input{nyx_plot.tex}
\input{nyx_map.tex}

%\vspace{-2mm}
\subsubsection{Jet Flow}
\label{sec:jet}
Table~\ref{jet_tab} lists all the Jet \textbf{Phase-II} test configurations.
%, Figure~\ref{fig:dataset_timings} contains computation costs, and Figure~\ref{fig:jet_plot} shows the distribution of trajectory reconstruction error for each test.
%
\input{jet_table.tex}

\textbf{Execution Time.} For the Jet data set, Lagrangian$_{Local}$ computed a flow map up to 3.9x faster than the corresponding Lagrangian$_{Dist}$ flow map.
%
For this data set, we considered shorter storage intervals, and thus a smaller percentage of particles required particle exchange to continue trajectory integration.
%
However, Figure~\ref{fig:dataset_timings} shows the variability in the cost of communication as it was susceptible to network usage and bandwidth contention.
%
A single outlier Lagrangian$_{Local}$ test showed a higher computation cost. 

\input{jet_plot.tex}
\input{jet_map.tex}
\textbf{Reconstruction Accuracy.} The Jet data set presented an adversarial case for our proposed optimization of computing local flow maps.
%
The data set contained regions with high velocity magnitude.
%
Across the range of configurations in Figure~\ref{fig:jet_plot}, both the storage interval and the data reduction factor impacted the reconstruction accuracy.
%
6 of 9 tests had a mean reconstruction error under 100\% of GCS. 
%
Further investigation into the distribution of T2 revealed that the longer storage interval and 1:1 sampling resulted in several particles terminating in easy-to-reconstruct areas of the domain at the start of the simulation (most other configurations showed particle termination only after cycle 30).
%
For tests using a storage interval of 10~(T2, T4, T6, T8), the reconstruction accuracy is reduced.
%
%The sparser sampling for other configurations with the same storage interval missed this effect.
%
Figure~\ref{jet_ftle} visualizes an FTLE field derived using the T2 Lagrangian$_{Local}$ flow map and annotates regions of reduced quality reconstruction.
%
However, since the absolute number of particles stored for this test remained high at 97.9\%, most of the FTLE field structure was well reconstructed.
%
%Further, as the number of samples used reduces, the reconstruction of discarded trajectories can be multiple cells away from the ground truth.
%
%For example, T8 using 1:64 data reduction factor and a storage interval of 10, has a mean reconstruction of 150% of GCS.

\textbf{Impact of Varying Data Reduction Factor.} Figure~\ref{fig:jet_map} compares the distribution of reconstruction error as the data reduction factor increases. 
%
Each cell in the heatmap shows a percentage of particles to enable comparison between tests.
%
For test T1 using a 1:1 sampling, reconstruction accuracy was high and a larger percentage of particles were reconstructed with error under 100\% of GCS.
%
As the number of particles used to sample the domain reduced, a higher percentage of particles were reconstructed multiple cells away from the ground truth. 
%
For example, in Figure~\ref{fig:jet_7} test T7 used a 1:64 data reduction factor and there are more red/orange cells with higher error during the later stages of the simulation. 
%
%Based on the distribution of error, the reconstruction accuracy is high when a 1:1 data reduction factor is used, and reduces in accuracy with fewer samples.
%
%Overall, as the number of particles used to sample the domain reduces the mean reconstruction error increases, with tests using a longer storage interval showing higher error.
%
%
%Overall, based on the distribution of reconstruction accuracy, the probability of accurate reconstruction increases with more samples.

